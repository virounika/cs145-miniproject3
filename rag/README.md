# **CS 145 Mini Project 3 - README**


## **ğŸš€ Overview**
This project implements a **Retrieval-Augmented Generation (RAG) system** for the **Meta KDD Cup 2024 CRAG challenge**. The system integrates **vLLM**, **Meta-Llama 3.2-3B-Instruct**, and **Google Cloud VM** to efficiently answer complex questions using an LLM-powered pipeline.

---

## **ğŸ“ Project Structure**

```
CS_145_Mini_Project_3/
â”‚â”€â”€ rag/                     # Codebase for RAG system
â”‚   â”œâ”€â”€ generate.py          # Generates model predictions
â”‚   â”œâ”€â”€ evaluate.py          # Evaluates model performance
â”‚   â”œâ”€â”€ evaluation_model.py  # Evaluates model performance
â”‚   â”œâ”€â”€ rag_baseline.py      # RAG model implementation
â”‚   â”œâ”€â”€ vanilla_baseline.py  # Vanilla baseline implementation
â”‚â”€â”€ data/                    # Contains dataset files
â”‚   â”œâ”€â”€ crag_task_1_dev_v4_release.jsonl.bz2
â”‚â”€â”€ output/                  # Stores generated predictions & evaluation results
â”‚â”€â”€ requirements.txt         # Python dependencies
â”‚â”€â”€ README.md                # This file
```

---

## **ğŸ”¹ Prerequisites**
### **1ï¸âƒ£ Google Cloud VM Setup**
1. Go to [Google Cloud Compute Engine](https://console.cloud.google.com/compute/instances).
2. Create a new VM instance with:
   - **Machine Type:** `n1-standard-4` 
   - **GPU:** Tesla T4
   - **Boot Disk:** Deep Learning VM (CUDA 12.1)
   - **Firewall:** Allow HTTP & HTTPS
3. **Connect via SSH:**
   ```bash
   ssh -i ~/.ssh/my-gcloud-key your-username@YOUR_VM_EXTERNAL_IP
   ```

### **2ï¸âƒ£ Install Dependencies**
1. **Clone the repository & navigate to the project folder:**
   ```bash
   git clone https://github.com/YOUR_GITHUB_USERNAME/YOUR_REPO_NAME.git
   cd CS_145_Mini_Project_3
   ```
2. **Install dependencies:**
   ```bash
   pip install -r rag/requirements.txt
   pip install --upgrade openai 
   ```

### **3ï¸âƒ£ Authenticate with Hugging Face**
1. **Log in to Hugging Face:**
   ```bash
   huggingface-cli login
   ```
2. **Verify model access:**
   ```bash
   python -c "from transformers import AutoModel; model = AutoModel.from_pretrained('meta-llama/Llama-3.2-3B-Instruct')"
   ```
   âœ… No errors mean authentication is successful.

---

## **ğŸš€ Running the Model**
### **1ï¸âƒ£ Start vLLM Server** (Keep this running in a separate terminal)
```bash
export CUDA_VISIBLE_DEVICES=0
vllm serve meta-llama/Llama-3.2-3B-Instruct \
    --gpu_memory_utilization=0.85 \
    --tensor_parallel_size=1 \
    --dtype="half" \
    --max_model_len 8192 \
    --port=8088 \
    --enforce-eager
```
âœ… **Leave this running** while making API calls.

### **2ï¸âƒ£ Generate Model Predictions**
ğŸ“Œ Open a **new terminal** and run:
```bash
cd ~/CS_145_Mini_Project_3/rag
python generate.py \
    --dataset_path "data/crag_task_1_dev_v4_release.jsonl.bz2" \
    --split 1 \
    --model_name "rag_baseline" \
    --llm_name "meta-llama/Llama-3.2-3B-Instruct" \
    --is_server \
    --vllm_server "http://localhost:8088/v1"
```
âœ… This generates responses and saves them in `output/`.

### **3ï¸âƒ£ Evaluate the Model**
```bash
python evaluate.py \
    --dataset_path "data/crag_task_1_dev_v4_release.jsonl.bz2" \
    --model_name "rag_baseline" \
    --llm_name "meta-llama/Llama-3.2-3B-Instruct" \
    --is_server \
    --vllm_server "http://localhost:8088/v1" \
    --max_retries 10
```
âœ… This computes accuracy, hallucination rate, and missing rate.

---

## **ğŸ“‚ Output Files**
After execution, the output files will be located in:
```
CS_145_Mini_Project_3/output/rag_baseline/meta-llama-3.2-3B-Instruct/
â”‚â”€â”€ predictions.json          # Generated answers
â”‚â”€â”€ evaluation_results.json   # Accuracy and performance metrics
```
âœ… **Download these files for submission.**


